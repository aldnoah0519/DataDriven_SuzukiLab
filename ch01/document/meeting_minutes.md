# ${\mathbf U}$ や ${\mathbf V}$ の解釈について

###  データ ${\mathbf X}$ を表現するパターン（基底）のあつまり  
空間的パターン，というキーワードが出たのでそういう文脈で説明します．  

${\mathbf X} = ({\mathbf x} _ 1, {\mathbf x} _ 2, \cdots, {\mathbf x} _ m) \in \mathbb{R} ^ {n \times m}$ とします．
${\mathbf x} _ i \in \mathbb{R}^n$ は時刻 $i$ （ $i \Delta t$ のような解釈でもよい）における空間的パターンを表しているとします．  

空間パターン間に相関があれば，少数の支配的なパターン（基底）の組合せ（線形結合）でデータ ${\mathbf X}$ の各列を表現できそうです．
たとえば，極端な場合としてすべての $i$ について ${\mathbf x} _ i = (1,1,1)^\top$ であれば，
列数に関係なく ${\mathbf u} := (1,1,1)^\top$ だけで ${\mathbf X}$ を表現できるわけです．
SVD はこのような支配的パターンを抽出する系統立った方法と考えることができます．

$r$ 個の空間パターン $\mathcal{B} := ({\mathbf y} _ i) _ {i=1}^r$ の線形結合のみを使って ${\mathbf X}$ の各列を表現したいとします．
これをある意味で最も良く達成するのが $\tilde{\mathbf U}$ （ ${\mathbf U}$ の 1 列目から $r$ 列目までのスライス）の列の集まりです．
「最も良く」の意味は， $\mathcal{B}$ の線形結合を用いて ${\mathbf x} _ i$ を近似したときに発生する二乗誤差の，全データ（ $m$ 個）についての総和を最小化できるということです．
このパラグラフで言っていることは Theorem 1.1 の言い換えです．

要約すると，より支配的な空間的パターンが左側の列に並ぶようにしたものが ${\mathbf U}$ であるということです．  

${\mathbf X}$ の随伴 ${\mathbf X} ^ \ast$ をとると，その各列は（空間内のある座標における）時間的パターンに対応します．
${\mathbf X}^\ast$ に対しては， ${\mathbf V}$ が上述の ${\mathbf U}$ の役割をします．
つまり，より支配的な時間的パターンが左側の列に並ぶようにしたものが ${\mathbf V}$ です． 

### 射影
full SVD を ${\mathbf X}={\mathbf U} {\mathbf \Sigma} {\mathbf V}^\ast$ とします．
$\sigma_1, \sigma_2, \cdots , \sigma_r \neq 0, \ \sigma_i = 0 \ (i > r)$ とします．
${\mathbf U} = (\tilde{\mathbf U},\tilde{\mathbf U}^\perp)$ とし，
$\tilde{\mathbf U}$ は $r$ 本の列からなるとします．

ある空間的パターン ${\mathbf x} \in \mathbb{R}^n$ があったとします．
${\rm col}(\tilde{\mathbf U})={\rm col}({\mathbf X}), {\rm col}(\tilde{\mathbf U}^\perp) = {\rm ker}({\mathbf X}^\ast)$
および ${\mathbf U}$ のユニタリ性から，
$$\tilde{\mathbf U}^\ast {\mathbf x} = (\tilde{\mathbf U}^\ast {\mathbf x},{\tilde{\mathbf U}^\perp}^\ast {\mathbf x})^\top$$ 
における $\tilde{\mathbf U}^\ast {\mathbf x}, {\tilde{\mathbf U}^\perp}^\ast {\mathbf x}$ は
それぞれ ${\mathbf x}$ を ${\rm col}({\mathbf X}), {\rm ker}({\mathbf X}^\ast)$ に射影したときの成分になります．
${\rm col}({\mathbf X})$ に射影した成分に対応する基底は $\tilde{\mathbf U}$ の列ベクトルです．
したがって $\tilde{\mathbf U}\tilde{\mathbf U}^\ast {\mathbf x}$ は ${\mathbf x}$ を ${\rm col}({\mathbf X})$ に射影したベクトルです． 

# p. 31 でやっていること

まず訓練集合を用いて，顔画像の支配的なパターン ${\mathbf U}$ を抽出しています．
この支配的なパターン達の張る空間に，訓練集合に含まれていない新しい画像群（テスト集合）を射影しています．

テスト集合を射影して得られる画像が元画像と大きく食い違っていれば，テスト集合には支配的なパターンであると期待していた ${\mathbf U}$ に含まれないようなパターンがあることがわかります．
つまり，パターンの集まりである ${\mathbf U}$ は顔画像の多様性を十分に捉えられていないことになります． 

この画像は 32256 次元のベクトルですが，SVD から得られる 400 個程度の支配的なパターンの線形結合によって顔画像の多様性が捉えられているよ，ということをこの実験結果は示しています．

この結果は，この実験に使ったような 32256 次元の顔画像データは 400 次元程度のデータに圧縮できそうだよ，ということも言っています．

# 時系列データに SVD は使うのか

Figure 1.24 にあるように，小さな特異値に対応する空間的パターンは小さなノイズのようにふるまうことが多いので，観測時系列からノイズを除去するような目的で用いることがあります．

また，分類器といった入力で時系列を扱うための前処理として，時系列からの特徴量抽出手法として用いることがあります．
時系列を何度か観測するとします． $i$ 番目の観測時系列を ${\mathbf x} _ i$ としましょう．
これらをまとめたデータ ${\mathbf X} = ({\mathbf x} _ 1, {\mathbf x} _ 2, \cdots, {\mathbf x} _ m)$ から $\tilde{\mathbf U}$ を計算して，
$\tilde{\mathbf U}^\ast {\mathbf x}$ で得られるベクトルを特徴量として時系列を分類したりクラスタリングすることがあります．

さらに，この本の後半に POD という手法がでてきますが，これは空間的に支配的なパターンを SVD で抽出して，その線形結合で偏微分方程式が時間発展する舞台となる場を縮約表現する手法です．

# random SVD, テンソル分解よくわからん

わたしもよくわかりません．

* random SVD  
random SVD はアルゴリズムの気持ちはだいたいわかると思います
（QR 分解で得られる低ランク基底と，特異値の減衰の関係はちゃんとわかっていません．誰か勉強したら教えてください）．
power iteration は常識的な手法なので気持ちをわかってほしいです．Error bound とかの詳細は文献読まないとわからないです．

* テンソル分解  
テクニカルな用語の定義は一応全部 [これ](https://www.cs.umd.edu/class/fall2018/cmsc498V/slides/TensorBasics.pdf) で確認しましたが内容が分かった気には特にならなかったです．
動機はわかりますが，SVD みたいな最良性があるのか，とかそういう記述が一切ないのがアレですね．行列の場合の dyadic sum の素朴な拡張っぽいというのはわかるのですが．
テンソルのフロベニウスノルムとか unfolding は議論で使ってないのでなんのために導入してるのかわからないですね．後の章ででてくるのかもしれませんが．